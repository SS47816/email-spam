{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0af2904e-7863-4612-a320-a4919eb4db68",
   "metadata": {},
   "source": [
    "# EE5907/EE5027 Programming Assignment CA1\n",
    "\n",
    "> by: SUN Shuo A0162488U\n",
    "> \n",
    "> \"You may just run the code blocks all the way till the end\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f80225-3bcf-48ea-a11d-fe8a385ceb7b",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e473df4a-bbdb-4191-9efa-2d6568133b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train <class 'numpy.ndarray'> shape: (3065, 57) dtype: float64\n",
      "y train <class 'numpy.ndarray'> shape: (3065, 1) dtype: uint8\n",
      "X test <class 'numpy.ndarray'> shape: (1536, 57) dtype: float64\n",
      "y test <class 'numpy.ndarray'> shape: (1536, 1) dtype: uint8\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from time import sleep\n",
    "import sys\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load mat data\n",
    "mat_data = scipy.io.loadmat('data/spamData.mat')\n",
    "#print(mat_data)\n",
    "x_train = mat_data['Xtrain']\n",
    "y_train = mat_data['ytrain']\n",
    "x_test = mat_data['Xtest']\n",
    "y_test = mat_data['ytest']\n",
    "\n",
    "#x_train = np.array([(1,0), (1,1), (0,0)]).reshape(-1,2)\n",
    "#y_train = np.array([1, 1, 0]).reshape(-1,1)\n",
    "#x_test = np.array([(1,0), (1,0)]).reshape(-1,2)\n",
    "#y_test = np.array([1, 1]).reshape(-1,1)\n",
    "\n",
    "# Check data shapes and types\n",
    "print(\"X train\", type(x_train), \"shape:\", x_train.shape, \"dtype:\", x_train.dtype)\n",
    "print(\"y train\", type(y_train), \"shape:\", y_train.shape, \"dtype:\", y_train.dtype)\n",
    "print(\"X test\", type(x_test), \"shape:\", x_test.shape, \"dtype:\", x_test.dtype)\n",
    "print(\"y test\", type(y_test), \"shape:\", y_test.shape, \"dtype:\", y_test.dtype)\n",
    "\n",
    "# Binarization \n",
    "x_train_bin = (x_train > 0) * 1\n",
    "x_test_bin = (x_test > 0) * 1\n",
    "#print(x_train_bin)\n",
    "#print(x_test_bin)\n",
    "\n",
    "# Log Transform\n",
    "x_train_log = np.log(x_train + 0.1)\n",
    "x_test_log = np.log(x_test + 0.1)\n",
    "#print(x_train_log)\n",
    "#print(x_test_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ca288-86b1-4ea3-b7f6-254a6b22e0a4",
   "metadata": {},
   "source": [
    "## Q1. Beta-binomial Naive Bayes (24%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f3a72fd-d423-4c2a-9760-29f9793fe917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta(N, N_1, a, b):\n",
    "    \"\"\"\n",
    "    Compute the Beta(`alpha`, `alpha`) distribution\n",
    "    \"\"\"\n",
    "    if (N + a + b) > 0:\n",
    "        return (N_1 + a)/(N + a + b)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def featureLikelihood(X_train, Y_train, j, x_j, c, alpha):\n",
    "    \"\"\"\n",
    "    Compute the feature likelihood term for one (x_test, y_test=c) data point\n",
    "    Class: `c`, Feature: `j`: p(x_test_j| x_i_j, y_test=c)\n",
    "    \"\"\"\n",
    "    N = (Y_train == c).sum()\n",
    "    X_train_j = X_train[:, j].reshape(-1, 1)\n",
    "    N_1 = (X_train_j[Y_train == c] == 1).sum()\n",
    "    #print(\"N:\", N, \"N_1:\", N_1)\n",
    "    \n",
    "    if x_j == 1:\n",
    "        return beta(N, N_1, alpha, alpha)\n",
    "    else:\n",
    "        return 1 - beta(N, N_1, alpha, alpha)\n",
    "\n",
    "def posteriorPredictiveDistribution(X_train, Y_train, X_test, i, c, alpha):\n",
    "    \"\"\"\n",
    "    Compute the posterior predictive distribution of test feature\n",
    "    SUM of log(p(x_test_j | x_i_j, y_test=c))\n",
    "    \"\"\"\n",
    "    p_sum = 0\n",
    "    # For its j-th feature\n",
    "    for j in range(X_test.shape[1]):\n",
    "        p = featureLikelihood(X_train, Y_train, j, X_test[i][j], c, alpha)\n",
    "        if p > 0:\n",
    "            p_sum += np.log(p)\n",
    "        #print(\"Term(\", i, \",\", j, \") is:\", p)\n",
    "            \n",
    "    return p_sum\n",
    "    \n",
    "def betaBinomialNaiveBayes(X_train, Y_train, X_test, alpha):\n",
    "    \"\"\"\n",
    "    Fit a Beta Binomial Naive Bayes Classifier on the `X_train`, `Y_train` data,\n",
    "    and predict the results `Y_pred` with the given `alpha`\n",
    "    \"\"\"\n",
    "    # Class label prior lambda\n",
    "    lambda_ml = (Y_train == 1).sum() / Y_train.shape[0]\n",
    "    #print(\"lambda_ml:\", lambda_ml)\n",
    "    \n",
    "    Y_pred = np.zeros((X_test.shape[0], 1), dtype=int)\n",
    "    # For the i-th test data\n",
    "    for i in range(Y_pred.shape[0]):\n",
    "        P_0 = np.log(1 - lambda_ml) + posteriorPredictiveDistribution(X_train, Y_train, X_test, i, 0, alpha)\n",
    "        P_1 = np.log(lambda_ml) + posteriorPredictiveDistribution(X_train, Y_train, X_test, i, 1, alpha)\n",
    "        #print(P_0)\n",
    "        #print(P_1)\n",
    "        if P_0 < P_1:\n",
    "            Y_pred[i][0] = 1\n",
    "        #print(Y_pred)\n",
    "        #print(\"y predict\", type(Y_pred), \"shape:\", Y_pred.shape, \"dtype:\", Y_pred.dtype)\n",
    "    \n",
    "    return Y_pred\n",
    "\n",
    "def computeErrorRate(X_train, Y_train, X_test, Y_test, alpha):\n",
    "    \"\"\"\n",
    "    Compute the Error Rate based on the `Y_pred` result and the given ground truth `Y_test`, \n",
    "    with a given alpha values\n",
    "    \"\"\"\n",
    "    Y_pred = betaBinomialNaiveBayes(X_train, Y_train, X_test, alpha)\n",
    "    num_error = (Y_pred != Y_test).sum()\n",
    "    \n",
    "    return num_error/Y_test.shape[0]\n",
    "    \n",
    "def compareAlphas(X_train, Y_train, X_test, Y_test, alphas):\n",
    "    \"\"\"\n",
    "    Compute the Error Rate based on the `Y_pred` result and the given ground truth `Y_test`, \n",
    "    with varying alpha values\n",
    "    \"\"\"\n",
    "    n = alphas.shape[0]\n",
    "    error_rates = np.zeros((n, 1))\n",
    "    for i in range(n):\n",
    "        error_rates[i] = computeErrorRate(X_train, Y_train, X_test, Y_test, alphas[i])\n",
    "        # Print progress bar\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write(\"[{:{}}] {:.1f}%\".format(\"=\"*i, n-1, (100/(n-1)*i)))\n",
    "        sys.stdout.flush()\n",
    "        #sleep(0.01)\n",
    "        \n",
    "    return error_rates\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6f9d8c-56b2-4a85-994a-3d46b8aed5d3",
   "metadata": {},
   "source": [
    "### Compute and Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef0ee6c4-6832-4d9a-91bf-ebf26097ad05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                                                                                                                                                                                        ] 0.0%"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-02f07971fbdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Set experimenting alpha values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0malphas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_error_rates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompareAlphas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_error_rates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompareAlphas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-a91b952a09df>\u001b[0m in \u001b[0;36mcompareAlphas\u001b[0;34m(X_train, Y_train, X_test, Y_test, alphas)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0merror_rates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0merror_rates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputeErrorRate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;31m# Print progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-a91b952a09df>\u001b[0m in \u001b[0;36mcomputeErrorRate\u001b[0;34m(X_train, Y_train, X_test, Y_test, alpha)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0malpha\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \"\"\"\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbetaBinomialNaiveBayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mnum_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-a91b952a09df>\u001b[0m in \u001b[0;36mbetaBinomialNaiveBayes\u001b[0;34m(X_train, Y_train, X_test, alpha)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mP_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlambda_ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposteriorPredictiveDistribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mP_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposteriorPredictiveDistribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;31m#print(P_0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m#print(P_1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-a91b952a09df>\u001b[0m in \u001b[0;36mposteriorPredictiveDistribution\u001b[0;34m(X_train, Y_train, X_test, i, c, alpha)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# For its j-th feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatureLikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mp_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-a91b952a09df>\u001b[0m in \u001b[0;36mfeatureLikelihood\u001b[0;34m(X_train, Y_train, j, x_j, c, alpha)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mClass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mc\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFeature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mj\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_j\u001b[0m\u001b[0;34m|\u001b[0m \u001b[0mx_i_j\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \"\"\"\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mX_train_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mN_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train_j\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mY_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     45\u001b[0m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[1;32m     46\u001b[0m          initial=_NoValue, where=True):\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m def _prod(a, axis=None, dtype=None, out=None, keepdims=False,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set experimenting alpha values\n",
    "alphas = np.arange(0, 100.5, 0.5)\n",
    "train_error_rates = compareAlphas(x_train_bin, y_train, x_train_bin, y_train, alphas)\n",
    "test_error_rates = compareAlphas(x_train_bin, y_train, x_test_bin, y_test, alphas)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots()\n",
    "line1, = ax.plot(alphas, train_error_rates, label='training')\n",
    "line2, = ax.plot(alphas, test_error_rates, dashes=[6, 2], label='test')\n",
    "\n",
    "ax.set(xlabel='alpha', ylabel='error rate', title='Q1. Beta-binomial Naive Bayes')\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(\"pics/q1.png\")\n",
    "plt.show()\n",
    "\n",
    "# Print some results\n",
    "print(\"On the training set, the error rates for α = 1, 10, 100 are respectively:\", \n",
    "      train_error_rates[2], \",\", train_error_rates[20], \",\", train_error_rates[-1])\n",
    "print(\"On the test set, the error rates for α = 1, 10, 100 are respectively:\", \n",
    "      test_error_rates[2], \",\", test_error_rates[20], \",\", test_error_rates[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de55940a-bd42-4902-8c60-cd6fdc061cb9",
   "metadata": {},
   "source": [
    "## Q2. Gaussian Naive Bayes (24%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce86e502-b422-434a-8c2d-f7ecf11d4409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, mu, sigma_sq):\n",
    "    \"\"\"\n",
    "    Compute the gaussian(`mu`, `sigma_sq`) distribution of `x`\n",
    "    \"\"\"\n",
    "    if sigma_sq > 0:\n",
    "        return 1/np.sqrt(2*np.pi*sigma_sq) * np.exp(-0.5*np.power((x - mu), 2)/sigma_sq)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def paramMLEstimate(X_train, Y_train, c):\n",
    "    \"\"\"\n",
    "    Compute the ML estimate of `mean` and `var` for each feature\n",
    "    \"\"\"\n",
    "    row_idxs = []\n",
    "    for i in range(Y_train.shape[0]):\n",
    "        if Y_train[i][0] == c:\n",
    "            row_idxs.append(i)\n",
    "    X_train_c = X_train[np.array(row_idxs), :]\n",
    "    #print(\"X_train_c:\", X_train_c.shape)\n",
    "    mean = np.mean(X_train_c, axis=0)\n",
    "    var = np.var(X_train_c, axis=0)\n",
    "    #print(\"Mean:\", mean[0], \"shape:\", mean.shape)\n",
    "    #print(\"Var:\", var[0], \"shape:\", var.shape)\n",
    "    \n",
    "    return mean, var\n",
    "    \n",
    "def featureLikelihood(x_j, mu, sigma_sq):\n",
    "    \"\"\"\n",
    "    Compute the feature likelihood term for one (x_test, y_test=c) data point\n",
    "    Class: `c`, Feature: `j`: p(x_test_j| x_i_j, y_test=c)\n",
    "    \"\"\"\n",
    "    \n",
    "    return gaussian(x_j, mu, sigma_sq)\n",
    "    \n",
    "def sumFeatureLikelihood(X_test, Means, Vars, i, c):\n",
    "    \"\"\"\n",
    "    Compute the sum of test feature likelihood:\n",
    "    SUM(log(p(x_test_j | x_i_j, y_test=c)))\n",
    "    \"\"\"\n",
    "    p_sum = 0\n",
    "    for j in range(X_test.shape[1]):\n",
    "        p_sum += np.log(featureLikelihood(X_test[i][j], Means[c][j], Vars[c][j]))\n",
    "        \n",
    "    return p_sum\n",
    "\n",
    "def GaussianNaiveBayes(X_train, Y_train, X_test):\n",
    "    \"\"\"\n",
    "    Fit a Beta Binomial Naive Bayes Classifier on the `X_train`, `Y_train` data,\n",
    "    and predict the results `Y_pred` with the given `alpha`\n",
    "    \"\"\"\n",
    "    # Class label prior lambda\n",
    "    lambda_ml = (Y_train == 1).sum() / Y_train.shape[0]\n",
    "    #print(\"lambda_ml:\", lambda_ml)\n",
    "    Means = np.zeros((2, X_train.shape[1]))\n",
    "    Vars = np.zeros((2, X_train.shape[1]))\n",
    "    for i in range(2):\n",
    "        Means[i], Vars[i] = paramMLEstimate(X_train, Y_train, i)\n",
    "    #print(\"Means:\", Means, \"shape:\", Means.shape)\n",
    "    #print(\"Vars:\", Vars, \"shape:\", Vars.shape)\n",
    "    \n",
    "    Y_pred = np.zeros((X_test.shape[0], 1), dtype=int)\n",
    "    # For the i-th test data\n",
    "    for i in range(Y_pred.shape[0]):\n",
    "        P_0 = np.log(1 - lambda_ml) + sumFeatureLikelihood(X_test, Means, Vars, i, 0)\n",
    "        P_1 = np.log(lambda_ml) + sumFeatureLikelihood(X_test, Means, Vars, i, 1)\n",
    "        #print(P_0)\n",
    "        #print(P_1)\n",
    "        if P_0 < P_1:\n",
    "            Y_pred[i][0] = 1\n",
    "            \n",
    "    #print(Y_pred)\n",
    "    #print(\"y predict\", type(Y_pred), \"shape:\", Y_pred.shape, \"dtype:\", Y_pred.dtype)\n",
    "    \n",
    "    return Y_pred\n",
    "\n",
    "def computeErrorRate(X_train, Y_train, X_test, Y_test):\n",
    "    \"\"\"\n",
    "    Compute the Error Rate based on the `Y_pred` result and the given ground truth `Y_test`, \n",
    "    with a given alpha values\n",
    "    \"\"\"\n",
    "    Y_pred = GaussianNaiveBayes(X_train, Y_train, X_test)\n",
    "    num_error = (Y_pred != Y_test).sum()\n",
    "    return num_error/Y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "623e293b-e6ca-467b-ba2c-d5be516e56d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-47-3686ff15f18b>:42: RuntimeWarning: divide by zero encountered in log\n",
      "  p_sum += np.log(featureLikelihood(X_test[i][j], Means[c][j], Vars[c][j]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the training set, the error rate is: 0.16574225122349104\n",
      "On the test set, the error rate is: 0.16015625\n"
     ]
    }
   ],
   "source": [
    "# Compute the train and test error rate\n",
    "train_error_rate = computeErrorRate(x_train_log, y_train, x_train_log, y_train)\n",
    "test_error_rate = computeErrorRate(x_train_log, y_train, x_test_log, y_test)\n",
    "\n",
    "# Print some results\n",
    "print(\"On the training set, the error rate is:\", train_error_rate)\n",
    "print(\"On the test set, the error rate is:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c78a97-7205-431a-8f20-cf8dc110a303",
   "metadata": {},
   "source": [
    "## Q3. Logistic regression (24%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90afebe3-140f-4328-a6be-cb9dfc6cb201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc8ca0f5-ee13-4d45-91e5-3e4f510683f8",
   "metadata": {},
   "source": [
    "## Q4. K-Nearest Neighbors (24%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926cd4f9-4256-4b1f-9640-e5a8c1fd16e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
